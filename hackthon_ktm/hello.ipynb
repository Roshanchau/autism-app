{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' to', ' you', ' a', ' the', ' it', ' me', '.', ',', ' us', ' him', ' this', ' them', ' that', ' my', ' your', ' more', ' people', ' his', ' for', ' an', ' all', ' in', ' something', ' her', ' their', ' some', ' and', ' our', ' one', '.\"', ' no', ' someone', '!', ' nothing', ' as', ' is', ' everyone', ',\"', ' \"', '?', ' not', ' on', ' (', ' what', \"'s\", ' these', ' so', ' I', ' from', ' out']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Move the model to CPU (this is the default, but we can be explicit)\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "# Tokenize input (no need to move to device since it's CPU by default)\n",
    "inputs = tokenizer(\"i want\", return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "# Forward pass through the model to get logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, return_dict=True)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get the logits for the last token (the token corresponding to the last input)\n",
    "last_token_logits = logits[:, -1, :]\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = torch.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "# Get the top 100 most probable next tokens \n",
    "top_100_probs, top_100_indices = torch.topk(probabilities, 50)\n",
    "\n",
    "# Decode the top 100 tokens and their probabilities\n",
    "top_100_tokens = [tokenizer.decode([idx]) for idx in top_100_indices[0]]\n",
    "# top_100_probs = top_100_probs[0].cpu().numpy()\n",
    "\n",
    "# # Print the top 100 tokens with their probabilities\n",
    "# for token, prob in zip(top_100_tokens, top_100_probs):\n",
    "#     print(f\"Token: {token}, Probability: {prob:.4f}\")\n",
    "print(top_100_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Move the model to CPU\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "# Load dataset from a text file\n",
    "\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        sentences = file.readlines()\n",
    "    return [sentence.strip() for sentence in sentences]\n",
    "\n",
    "# print(\"dataset\", load_da)\n",
    "\n",
    "\n",
    "# Specify the path to your dataset\n",
    "dataset_file = 'dataset.txt'  # Update this to your file's location\n",
    "dataset = load_dataset(dataset_file)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique next words for 'I want': ['the', 'icecream', 'to']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Load dataset from a text file\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        sentences = file.readlines()\n",
    "    # print(\"sentence\", sentences)\n",
    "    # print(\"Hehe\", [sentence.strip() for sentence in sentences])\n",
    "    return [sentence.strip() for sentence in sentences]\n",
    "\n",
    "# Function to get unique next words\n",
    "def get_unique_next_words(input_phrase, dataset):\n",
    "    next_words = set()  # Use a set to store unique words\n",
    "\n",
    "    # Process each sentence in the dataset\n",
    "    for sentence in dataset:\n",
    "        # Check if the sentence starts with the input phrase\n",
    "        if sentence.lower().startswith(input_phrase.lower()):\n",
    "            # Remove the input phrase from the sentence\n",
    "            remaining_text = sentence[len(input_phrase):].strip()\n",
    "\n",
    "            # Extract the first word after the input phrase\n",
    "            if remaining_text:\n",
    "                next_word = remaining_text.split()[0]  # Get the first word\n",
    "                next_words.add(next_word)  # Add to the set for uniqueness\n",
    "\n",
    "    return list(next_words)  # Convert set back to list for output\n",
    "\n",
    "# Example usage\n",
    "dataset_file = 'dataset.txt'  # Update this to your file's location\n",
    "dataset = load_dataset(dataset_file)\n",
    "\n",
    "input_text = \"I want\"\n",
    "predicted_next_words = get_unique_next_words(input_text, dataset)\n",
    "print(f\"Unique next words for '{input_text}':\", predicted_next_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
